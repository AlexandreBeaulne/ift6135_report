\documentclass{article}

\usepackage[final]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{float}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}

\usepackage{color}

\bibliographystyle{plainnat}
\setlength{\parskip}{0pt}

\def\defeq{\dot=}

\title{Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning}

\author{
  Alexandre Beaulne \\
  \texttt{alex@mgnr.io} \\
  ID 20087309
  \And
  Amina Madzhun \\
  \texttt{amina.madzhun@umontreal.ca} \\
  ID 20052277
}

\begin{document}

\maketitle
\section{Introduction}

Our project has for objective to reproduce a subset of results
the results from TD or Not TD: Analyzing the Role of Temporal
Differencing in Deep Reinforcement Learning \citep{amiranashvili2018analyzing}.
In recent years, techniques combining Reinforcement Learning (RL)
and Deep Learning (DL) have emerged \citep{mnih2015} to produce
superhuman performance at some control tasks. However, the behavior
of deep neural architectures when used as function approximators
in RL is still poorly understood. The chosen paper aims to improve
this understanding.

\subsection*{Background}

RL is an area of machine learning concerned with how software agents
ought to take actions in an environment so as to maximize some notion
of cumulative reward \citep{suttonbarto2018}. In the traditional RL
framework, an agent's evolution is \emph{entirely} defined by a sequence
of state, action and reward tuples. This sequence forms a
partially observable markov decision process (POMDP). The objective
of the agent is to pick the action in any state that will
maximize the cumulative rewards encountered till the end of the
sequence (the \emph{episode}).\\

More rigourously, a standard RL problem develops over discrete
time steps involving an episodic setup (starting an episode at
$t=0$ and finishing it at terminal time step $t=T$).
The agent evolves in a stochastic environment with its current state
$s_t$ at time step $t$, and can can perform an action $a_t$ from a
set of actions. This action leads the environment to a new state
$s_{t + 1}$, and gives a reward $r_t$ to the agent or finishes the episode.
The goal of the agent is to learn a policy $\pi(a_t|s_t)$
to obtain the maximum expected return over time which is
the total amount of reward that it can get from the current time step
to the end of the episode, with future rewards being reduced by discount
factor $\gamma$ for each time step. \\

For a given policy $\pi$ the value function and the action-value function
are defined as:
    \begin{eqnarray*}
    V^\pi(s_t) \defeq \mathbb{E}_{\pi} [\hat{R}_t | s_t], &
    Q^\pi(s_t, a_t) \defeq \mathbb{E}_{\pi} [\hat{R}_t | s_t, a_t]
    \end{eqnarray*}
where $\hat{R}_t$ is cumulated discounted reward

\begin{equation}
R^\gamma_t = \sum_{i=t}^T \gamma^{i-t} r_i
\end{equation}

or a truncated sum

\begin{equation}
R^\tau_t = \sum_{i=t}^{t+\tau} r_i
\end{equation}

$\tau$ in this case is a fixed number of steps; the \emph{horizon}.

The optimal state and state-action value functions are respectively given by

\begin{eqnarray} \label{optimal}
    V^*(s_t) \defeq \max_{\pi}V^\pi(s_t), &
    Q^*(s_t, a_t) \defeq \max_{\pi} Q^\pi(s_t, a_t)
\end{eqnarray}

Value based algorithms to find the optimal policy require computing
or approximating one of the value function from equation (\ref{optimal}).

\section{Paper Summary}
RL algorithms to find optimal policies involve exploring trajectories
and backtracking observed rewards to previous visited state-action pairs.
The depth and breath of the search vary between different algorithms
(Figure (\ref{backups})).

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{backups}
    \caption{Different backtracking schemes \citet{suttonbarto2018}}
    \label{backups}
\end{figure}

The large size of the state space of the problems considered preclude full breadth
search, which leaves sampling as the only viable exploration scheme. Sampling
search can be divided into two approaches, depending on the depth of the trajectory
sampled. Monte Carlo (MC) search samples a trajectory till the end, while temporal
differencing (TD) samples a single step, and approximate the value of the rest of the
trajectory with the current value function at the next state (i.e. \emph{bootstrap}).
Naturally, the complete spectrum of hybrid exploration depths, i.e. rollout length,
from 1 step (TD) to the end of the episode (MC) is possible.
Before the advent of deep network as function approximators, it has been empirically
demonstrated that TD is superior to MC \citep{sutton1995}. However, it is unclear
whether those results still hold with the use of deep neural nets as function approximators.
This question is explored in \citep{amiranashvili2018analyzing}. \\

The authors perform two sets of experiments. In one set, they benchmark three different
algorithms (A3C, n-step Q-learning and $Q_MC$) on games from the Atari and VizDoom environments,
varying rollout lengths. The second set of experiments are \emph{controlled}, fixed algorithms
were tested against environments whose reward randomness, delay and sparsity were
carefully adjusted.\\

They conclude that MC estimation is a feasible and sometimes even competitive alternative
to TD. MC performs particularly well in visually complex 3D environments
and with delayed or sparse rewards. As for rollout, the authors obtain the
best results with a rollout length of 20.

\section{Experiments \& Discussion}

Considering the large computational resources necessary to train deep reinforcement models,
we set out to reproduce a subset of the original paper, namely running two out of the three
algorithms (n-step Q-learning and A3C) on two Atari games (Pong and Seaquest),
also measuring the impact of different rollout lengths. We were not able to reproduce
the $Q_{MC}$ algorithm. While pseudocode for n-step Q-learning and A3C is available in
\cite{pmlr-v48-mniha16}, \cite{amiranashvili2018analyzing} did not provide pseudocode
for $Q_{MC}$, their explaination were insufficient for us to be able to implement the
algorithm, and source code was not shared by the authors yet. \\

Table (\ref{table1}) shows average scores achieved by our implementations. Our
implementation of n-step Q-learning did not achieve performance better than random
on the game Seaquest. Our implemention of n-step Q-learning performed better than
the original article's for the game of Pong, and our implementation of A3C performed
better that the original article's for both the game of Pong and the game of Seaquest.

\begin{table}[H]
\centering
\label{table1}
    \begin{tabular}{@{}l|c|cc}
    \toprule
                                             & \#steps  & Seaquest  & Pong  \\ \midrule
    20-step A3C (Mnih et al., 2016)          & 80M      & 2300      & 11.4  \\ \midrule
    $Q_{MC}$ (Amiranashvili et al., 2017)    & 60M      & 12708     & -4.2  \\
    20-step $Q$ (Amiranashvili et al., 2017) & 60M      & 4276      & 8.9   \\
    20-step A3C (Amiranashvili et al., 2017) & 60M      & 2021      & 20.6  \\ \midrule
    20-step $Q$ (our results)                & 20M      & 100       & 21.0  \\
    20-step A3C (our results)                & 20M      & 2702      & 21.0  \\ \bottomrule
    \end{tabular}
\caption{Our results at two Atari games against selected publications. Higher scores are better for both games.}
\end{table}

Figure (\ref{rollouts}) shows the impact on score of different rollout lengths.
Our results align with \cite{amiranashvili2018analyzing}, with $n=20$ being
the optimal rollout length.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{results}
    \caption{Effect of rollout length on TD learning for n-step Q and A3C. Score on
    Pong and Seaquest Atari games. Higher is better.}
    \label{rollouts}
\end{figure}

Animations showing the different algorithm in action before and after training
are available at \url{http://cs.mcgill.ca/~abeaul10/rl_prez/animations.html}.

\subsection*{Model architecture details}
It is important to mention that for the implementation of deep nonlinear function 
approximators in the original work the authors used the same network architecture 
as for design of DQN by \citet{mnih2015}.
For learning from a raw image input (preprocessed stacked frames) the model 
consisting of 3 convolutional layers followed by a fully connected layer was giving
as an output the corresponding predicted Q-values for each valid action.

To try to improve the results we slightly modified this setting inspired by successful implementation
of Deep Recurrent Q-network (DRQN) by \citet{DBLP:journals/corr/HausknechtS15} for POMDP. The key idea
of this improvement is to liquidate the limitation of DQN which is based on 4 recent past states. 
As the solution for the necessity to have a memory for longer than 4 recent frames, 
a fully connected layer was replaced by a recurrent LSTM layer of the same size. 
This only difference resulted in better performance of the model to handle
the loss of information. In our implementation we used 4 convolution layers
with kernel 3x3, stride 2 and padding 1, followed by an LSTM layer.  

Figure (\ref{dnn}) might help to visually see the contrast between the architectures. 
\begin{figure}[h]
	\centering
	\includegraphics[width = 7.5cm,height = 6cm]{dqn}
	\includegraphics[width = 6cm]{drqn}
	\caption{Deep network architectures. Left: DQN. Right: DRQN 
	\label{dnn}
\end{figure}


\pagebreak

\small

\bibliography{main}

\end{document}

